{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qau4--1Xw7IW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import joblib\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load and preprocess data\n",
        "def load_and_preprocess_data(df):\n",
        "    # Create copy of dataframe\n",
        "    data = df.copy()\n",
        "\n",
        "    # Create feature matrix X and target vector y\n",
        "    X = data.drop('target', axis=1)\n",
        "    y = data['target']\n",
        "\n",
        "    # Split data into train and test sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42\n",
        "    )\n",
        "\n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "    return X_train_scaled, X_test_scaled, y_train, y_test, scaler\n",
        "\n",
        "# Train model\n",
        "def train_model(X_train, y_train):\n",
        "    # Initialize RandomForestClassifier with optimized hyperparameters\n",
        "    rf_model = RandomForestClassifier(\n",
        "        n_estimators=100,\n",
        "        max_depth=10,\n",
        "        min_samples_split=5,\n",
        "        min_samples_leaf=2,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    rf_model.fit(X_train, y_train)\n",
        "    return rf_model\n",
        "\n",
        "# Evaluate model\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Calculate feature importance\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_names,\n",
        "        'importance': model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "\n",
        "    print(\"\\nFeature Importance:\")\n",
        "    print(feature_importance)\n",
        "\n",
        "    return feature_importance\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "data = pd.read_csv('/content/Heart Disease dataset.csv')\n"
      ],
      "metadata": {
        "id": "z0Sc_alMxGPa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process data\n",
        "X_train_scaled, X_test_scaled, y_train, y_test, scaler = load_and_preprocess_data(data)\n",
        "feature_names = data.drop('target', axis=1).columns\n",
        "\n"
      ],
      "metadata": {
        "id": "wNlOltvixIoF"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "model = train_model(X_train_scaled, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "4mCFghnNxMJA"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model\n",
        "feature_importance = evaluate_model(model, X_test_scaled, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "muawNjVaxOGX",
        "outputId": "0b54ee70-f8a6-4203-f34f-f8be1a4c0069"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.83      0.86        29\n",
            "           1       0.85      0.91      0.88        32\n",
            "\n",
            "    accuracy                           0.87        61\n",
            "   macro avg       0.87      0.87      0.87        61\n",
            "weighted avg       0.87      0.87      0.87        61\n",
            "\n",
            "\n",
            "Feature Importance:\n",
            "     feature  importance\n",
            "9    oldpeak    0.140247\n",
            "11        ca    0.128733\n",
            "2         cp    0.120215\n",
            "12      thal    0.110517\n",
            "7    thalach    0.107038\n",
            "8      exang    0.081898\n",
            "0        age    0.079609\n",
            "3   trestbps    0.063337\n",
            "4       chol    0.062113\n",
            "10     slope    0.047390\n",
            "1        sex    0.038293\n",
            "6    restecg    0.014442\n",
            "5        fbs    0.006168\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model and scaler\n",
        "joblib.dump(model, 'heart_disease_model.joblib')\n",
        "joblib.dump(scaler, 'scaler.joblib')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSdQxpRKxQlV",
        "outputId": "27006192-fcad-4ba7-fc7c-5afa94395dae"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['scaler.joblib']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save feature names and their importance scores\n",
        "feature_importance.to_csv('feature_importance.csv', index=False)"
      ],
      "metadata": {
        "id": "kobjvdo1xSb4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6cs2H68dyG8j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}